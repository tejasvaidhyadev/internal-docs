---
title: Introduction
description: 'Automated Platform for Building Large Foundation Models'
---

<Frame>
  <img
    className="block dark:hidden"
    src="/logos/cube_exploded_hero.png"
    alt="Nolano.AI Logo"
  />
  <img
    className="hidden dark:block"
    src="/logos/cube_exploded_hero.png"
    alt="Nolano.AI Logo"
  />
</Frame>

# Nolano AI — Automated Platform for Building Large Foundation Models

<Note>
  **For AI/ML Researchers** at the frontier of foundation modeling research.
</Note>

The library seeks to abstract away the complexity of HPC, distributed training, tooling, systems and non-AI related engineering overhead involved in getting the foundation model research going.  
We allow AI researchers to focus on what matters most: **breakthrough innovations in foundation models**, by getting their experiment going in minutes, not months.

## What We Abstract Away

Our platform eliminates the boilerplate parts of foundation model research so you can focus on pushing the frontier:

<CardGroup cols={2}>
  <Card
    title="Engineering Setup"
    icon="wrench"
    color="#935095"
  >
    Months of engineering setup for distributed training across clusters
  </Card>
  <Card
    title="HPC Expertise" 
    icon="microchip"
    color="#B47BB6"
  >
    Deep expertise in HPC, CUDA, parallelization strategies, and hardware optimization
  </Card>
  <Card
    title="Data Pipelines"
    icon="database"
    color="#6F3A71"
  >
    Redundant implementation of data loaders, training loops, and evaluation pipelines
  </Card>
  <Card
    title="Configuration Management"
    icon="gear"
    color="#935095"
  >
    Complex configuration management across different experiments and modalities
  </Card>
</CardGroup>

## Supported Modalities

<Tabs>
  <Tab title="Language & Code">
    Build state-of-the-art language and code models with support for all major architectures.
  </Tab>
  <Tab title="Time Series">
    Create powerful forecasting models for both univariate and multivariate time series data.
  </Tab>
</Tabs>

### Key Features

<AccordionGroup>
  <Accordion title="Tokenization Support">
    - Custom tokenizers for text & time series
    - BPE, WordPiece, SentencePiece implementations
  </Accordion>

  <Accordion title="Architecture Support">
    - Major dense and MoE architectures
    - LLaMA, Qwen, DeepSeek, Mistral, Phi
  </Accordion>

  <Accordion title="Distributed Training">
    - Multi-node, multi-GPU training out of the box
    - Data parallelism, model parallelism, and pipeline parallelism
    - Gradient accumulation and mixed precision training
    - Automatic sharding and load balancing
  </Accordion>

  <Accordion title="Data Pipeline">
    - High-performance data loading and preprocessing
    - Memory-efficient streaming for large datasets
    - Built-in data validation and quality checks
    - Automatic data shuffling and batching
  </Accordion>

  <Accordion title="Training Optimization">
    - Adaptive learning rate scheduling
    - Gradient clipping and stability monitoring
    - Memory optimization techniques
    - Dynamic loss scaling for mixed precision
  </Accordion>

  <Accordion title="Evaluation & Monitoring">
    - Real-time training metrics and visualization
    - Comprehensive model evaluation suites
    - Custom evaluation metrics support
    - Tensorboard and Weights & Biases integration
  </Accordion>

  <Accordion title="Model Management">
    - Automatic checkpointing and versioning
    - Integration with Huggingface
  </Accordion>

  <Accordion title="Scalability & Performance">
    - Dynamic scaling based on workload
    - Optimized for cloud and on-premise deployments
    - GPU memory optimization and efficient utilization
    - Support for various hardware accelerators (A100, H100, V100)
  </Accordion>
</AccordionGroup>

## What You Get

<CardGroup cols={2}>
  <Card
    title="Zero-to-training in under an hour"
    icon="rocket"
    color="#935095"
    href="/quickstart"
  >
    Start your first experiment immediately with our streamlined workflow
  </Card>
  <Card
    title="One-command operations"
    icon="terminal"
    color="#B47BB6"
    href="/quickstart"
  >
    Data preparation, training, evaluation and inference with simple CLI commands
  </Card>
  <Card
    title="Built-in best practices"
    icon="star"
    color="#6F3A71"
    href="/training"
  >
    Scalable distributed training, hyperparameter transfer and automated optimization
  </Card>
  <Card
    title="Production-ready"
    icon="check"
    color="#935095"
    href="/evaluation"
  >
    Models ready for deployment without additional engineering overhead
  </Card>
</CardGroup>

<Note>
  Ready to get started? Check out our [Quickstart Guide](/quickstart) to begin building your first foundation model.
</Note>

---

*Nolano.AI — Democratizing Foundation Model Research*
