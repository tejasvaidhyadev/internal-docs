---
title: 'Model Configuration'
description: 'Configure model architecture and initialization'
---

## Model Configuration

### ModelConfig

Configuration for model architecture and initialization.

<ParamField path="architecture" type="str" required>
  Model architecture specification. Supports major dense and MoE Hugging Face architectures including Qwen, LLaMA, Gemma.
  
  <Note>
    TSFM and other architectures coming soon.
  </Note>
</ParamField>

<ParamField path="init_method" type="str" default="normal">
  Weight initialization strategy:
  - `"none"`: Load from pre-trained model (Qwen/LLaMA/Gemma)
  - `"normal"`: Normal distribution initialization
  - `"xavier_uniform"`: Xavier uniform initialization
  - `"wang_init"`: Wang initialization method
</ParamField>

<ParamField path="model_path" type="str or None" default="None">
  Path to pre-trained model for continual training. Must be `None` if `init_method` is not `"none"`.
</ParamField>

<ParamField path="load_optimizer" type="bool or None" default="None">
  Whether to load optimizer state from checkpoint. Set to `True` for continual training from checkpoint.
</ParamField>

<ParamField path="precision" type="str" default="fp16">
  Model precision configuration:
  - `"binary"`: Binary precision (1-bit)
  - `"ternary"`: Ternary precision (1.58-bit)
  - `"int2"`: 2-bit integer precision
  - `"fp8"`: 8-bit floating point
  - `"mxfp4"`: 4-bit microscaling floating point
  - `"mxfp6"`: 6-bit microscaling floating point
  - `"ue8m0"`: 8-bit unsigned integer with 0 exponent bits
  - `"fp16"`: 16-bit floating point (default)
  - `"fp32"`: 32-bit floating point
</ParamField>

## Example Configurations

<CodeGroup>
```python Pre-trained Model
from pynolano import ModelConfig

config = ModelConfig(
    architecture="Qwen/Qwen3-4B",
    init_method="none",
    model_path="./pretrained_model",
    load_optimizer=False,
    precision="fp16"
)
```

```python From Scratch
from pynolano import ModelConfig

config = ModelConfig(
    architecture="Qwen/Qwen3-4B",
    init_method="normal",
    precision="fp16"
)
```

```python Continual Training
from pynolano import ModelConfig

config = ModelConfig(
    architecture="Qwen/Qwen3-4B",
    init_method="none",
    model_path="./checkpoint/global_step_1000",
    load_optimizer=True,
    precision="fp16"
)
```

```python Low Precision Training
from pynolano import ModelConfig

config = ModelConfig(
    architecture="Qwen/Qwen3-4B",
    init_method="normal",
    precision="ternary"
)
```
</CodeGroup>

## Supported Architectures

<Tabs>
  <Tab title="Dense Models">
    - **LLaMA Series** 
    - **Qwen Series**
    - **Gemma Series**  
  </Tab>
  
  <Tab title="Mixture of Experts (MoE)">
    - **Qwen MoE**  
    - **DeepSeek**
    - **Mixtral** 
  </Tab>
  
  <Tab title="Time Series Foundation Models">
    - **TSFM**: Time Series Foundation Models
    - **Chronos**: Amazon Chronos models
    - **TimesFM**: Google TimesFM
  </Tab>
  
  <Tab title="Low Precision Models">
    - **TriLM**: Ternary Language Models with 1.58-bit quantization
    - **BiLM**: Binary Language Models with 1-bit quantization
    - **Arbitrary-precision**: Language Models with Arbitrary Precision
  </Tab>
</Tabs>

you can request custom architecture if needed

<Warning>
  When using `init_method="none"`, you must provide a valid `model_path`. The model path should point to a compatible pre-trained model or checkpoint.
</Warning>
