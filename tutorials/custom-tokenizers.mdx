---
title: 'Custom Tokenizers'
description: 'Build custom tokenization strategies for specialized data'
---

# Custom Tokenizers

Learn how to implement custom tokenization strategies for specialized data types and domains.

## When to Use Custom Tokenizers

<CardGroup cols={2}>
  <Card 
    title="Specialized Domains" 
    icon="microscope"
    color="#935095"
  >
    Scientific data, medical records, legal documents
  </Card>
  <Card 
    title="Novel Data Types" 
    icon="sparkles"
    color="#B47BB6"
  >
    Audio spectrograms, sensor data, custom embeddings
  </Card>
  <Card 
    title="Performance Optimization" 
    icon="bolt"
    color="#6F3A71"
  >
    Domain-specific compression or efficiency improvements
  </Card>
  <Card 
    title="Research Innovation" 
    icon="flask"
    color="#935095"
  >
    Testing new tokenization approaches
  </Card>
</CardGroup>

## Basic Custom Tokenizer

### Simple Function-Based Tokenizer

```python
import torch
import numpy as np

def simple_custom_tokenizer(text_data):
    """
    Simple custom tokenizer that converts text to character-level tokens
    """
    # Create character vocabulary
    chars = list(set(''.join(text_data)))
    char_to_idx = {char: idx for idx, char in enumerate(chars)}
    
    # Tokenize
    tokens = []
    for text in text_data:
        token_ids = [char_to_idx[char] for char in text]
        tokens.append(torch.tensor(token_ids, dtype=torch.long))
    
    return tokens

# Use in DataPreparationConfig
from pynolano import DataPreparationConfig

def build() -> DataPreparationConfig:
    return DataPreparationConfig(
        input_path="./custom_data.txt",
        output_path="./prepared_custom",
        tokenization=simple_custom_tokenizer
    )
```

### Class-Based Tokenizer

```python
class CustomDomainTokenizer:
    def __init__(self, vocab_size=10000, special_tokens=None):
        self.vocab_size = vocab_size
        self.special_tokens = special_tokens or ['<pad>', '<unk>', '<sos>', '<eos>']
        self.token_to_id = {}
        self.id_to_token = {}
        self._build_vocab()
    
    def _build_vocab(self):
        """Build vocabulary from training data"""
        # Add special tokens first
        for i, token in enumerate(self.special_tokens):
            self.token_to_id[token] = i
            self.id_to_token[i] = token
    
    def train(self, data):
        """Train tokenizer on your data"""
        # Extract domain-specific patterns
        # Build vocabulary based on frequency
        # This is where your domain expertise goes
        pass
    
    def encode(self, text):
        """Convert text to token IDs"""
        tokens = self._tokenize(text)
        return [self.token_to_id.get(token, self.token_to_id['<unk>']) 
                for token in tokens]
    
    def decode(self, token_ids):
        """Convert token IDs back to text"""
        tokens = [self.id_to_token.get(id, '<unk>') for id in token_ids]
        return self._detokenize(tokens)
    
    def _tokenize(self, text):
        """Your custom tokenization logic"""
        # Implement domain-specific tokenization
        pass
    
    def _detokenize(self, tokens):
        """Convert tokens back to text"""
        # Implement reverse tokenization
        pass
    
    def __call__(self, data):
        """Make the class callable for Nolano.AI"""
        if isinstance(data, str):
            return torch.tensor(self.encode(data), dtype=torch.long)
        elif isinstance(data, list):
            return [torch.tensor(self.encode(item), dtype=torch.long) 
                    for item in data]

# Usage
tokenizer = CustomDomainTokenizer()
tokenizer.train(your_training_data)

def build() -> DataPreparationConfig:
    return DataPreparationConfig(
        input_path="./domain_data",
        output_path="./prepared_domain",
        tokenization=tokenizer
    )
```

## Specialized Examples

### Scientific Data Tokenizer

```python
class ScientificTokenizer:
    """Tokenizer for scientific papers with special handling for formulas, citations, etc."""
    
    def __init__(self):
        self.formula_pattern = r'\$.*?\$|\\\[.*?\\\]'
        self.citation_pattern = r'\[[\d,\s-]+\]'
        self.unit_pattern = r'\d+\.?\d*\s*[a-zA-Z]+/?[a-zA-Z]*'
        
    def _preprocess(self, text):
        """Special preprocessing for scientific text"""
        import re
        
        # Extract and replace formulas with special tokens
        formulas = re.findall(self.formula_pattern, text)
        for i, formula in enumerate(formulas):
            text = text.replace(formula, f'<FORMULA_{i}>', 1)
        
        # Handle citations
        citations = re.findall(self.citation_pattern, text)
        for i, citation in enumerate(citations):
            text = text.replace(citation, f'<CITATION_{i}>', 1)
        
        # Handle units
        units = re.findall(self.unit_pattern, text)
        for i, unit in enumerate(units):
            text = text.replace(unit, f'<UNIT_{i}>', 1)
        
        return text, {'formulas': formulas, 'citations': citations, 'units': units}
    
    def __call__(self, text):
        processed_text, metadata = self._preprocess(text)
        # Continue with standard tokenization...
        return tokens
```

### Time Series Custom Tokenizer

```python
class MultimodalTimeSeriesTokenizer:
    """Custom tokenizer for time series with categorical features"""
    
    def __init__(self, value_bins=1000, categorical_vocab=None):
        self.value_bins = value_bins
        self.categorical_vocab = categorical_vocab or {}
        
    def _quantize_values(self, values):
        """Convert continuous values to discrete bins"""
        min_val, max_val = values.min(), values.max()
        bins = np.linspace(min_val, max_val, self.value_bins)
        return np.digitize(values, bins)
    
    def _encode_categorical(self, categories):
        """Encode categorical features"""
        encoded = []
        for cat in categories:
            if cat in self.categorical_vocab:
                encoded.append(self.categorical_vocab[cat])
            else:
                # Add new category
                new_id = len(self.categorical_vocab)
                self.categorical_vocab[cat] = new_id
                encoded.append(new_id)
        return encoded
    
    def __call__(self, data):
        """
        data format: {'values': [1.2, 1.5, ...], 'categories': ['A', 'B', ...]}
        """
        values = np.array(data['values'])
        categories = data.get('categories', [])
        
        # Quantize time series values
        quantized_values = self._quantize_values(values)
        
        # Encode categorical features
        encoded_categories = self._encode_categorical(categories)
        
        # Combine into single sequence
        # Values get IDs 0-999, categories get IDs 1000+
        combined_tokens = list(quantized_values)
        combined_tokens.extend([id + self.value_bins for id in encoded_categories])
        
        return torch.tensor(combined_tokens, dtype=torch.long)
```

### Audio Spectrogram Tokenizer

```python
class SpectrogramTokenizer:
    """Tokenizer for audio spectrograms treated as 2D patches"""
    
    def __init__(self, patch_size=(16, 16), vocab_size=8192):
        self.patch_size = patch_size
        self.vocab_size = vocab_size
        self.patch_vocab = {}  # Will be learned
        
    def _extract_patches(self, spectrogram):
        """Extract patches from 2D spectrogram"""
        import torch.nn.functional as F
        
        # Assume spectrogram is [freq_bins, time_steps]
        h, w = spectrogram.shape
        ph, pw = self.patch_size
        
        # Pad if necessary
        pad_h = (ph - h % ph) % ph
        pad_w = (pw - w % pw) % pw
        spectrogram = F.pad(spectrogram, (0, pad_w, 0, pad_h))
        
        # Extract patches
        patches = spectrogram.unfold(0, ph, ph).unfold(1, pw, pw)
        return patches.reshape(-1, ph, pw)
    
    def _quantize_patches(self, patches):
        """Convert patches to discrete tokens using learned vocabulary"""
        # This would involve clustering/VQ-VAE-style quantization
        # For simplicity, using patch mean as proxy
        patch_means = patches.mean(dim=(1, 2))
        
        # Quantize means into bins
        bins = torch.linspace(patch_means.min(), patch_means.max(), self.vocab_size)
        token_ids = torch.searchsorted(bins, patch_means)
        
        return token_ids
    
    def __call__(self, spectrogram):
        """Convert spectrogram to token sequence"""
        patches = self._extract_patches(spectrogram)
        tokens = self._quantize_patches(patches)
        return tokens.long()
```

## Integration with Nolano.AI

### Complete Example: Medical Report Tokenizer

```python
import re
import torch
from transformers import AutoTokenizer

class MedicalReportTokenizer:
    """Specialized tokenizer for medical reports"""
    
    def __init__(self, base_tokenizer="microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"):
        self.base_tokenizer = AutoTokenizer.from_pretrained(base_tokenizer)
        
        # Medical-specific patterns
        self.vital_signs_pattern = r'BP:\s*\d+/\d+|HR:\s*\d+|Temp:\s*\d+\.?\d*'
        self.medication_pattern = r'\b[A-Z][a-z]+(?:ol|ine|cin|ide|ril|tan)\b'
        self.icd_pattern = r'[A-Z]\d{2}\.?\d*'
        
        # Special medical tokens
        self.special_tokens = ['<VITAL>', '<MEDICATION>', '<ICD>', '<LAB_VALUE>']
        self.base_tokenizer.add_tokens(self.special_tokens)
    
    def _preprocess_medical_text(self, text):
        """Extract and normalize medical entities"""
        # Replace vital signs
        text = re.sub(self.vital_signs_pattern, '<VITAL>', text)
        
        # Replace medications
        text = re.sub(self.medication_pattern, '<MEDICATION>', text)
        
        # Replace ICD codes
        text = re.sub(self.icd_pattern, '<ICD>', text)
        
        return text
    
    def __call__(self, text):
        """Tokenize medical text"""
        if isinstance(text, list):
            return [self._tokenize_single(item) for item in text]
        else:
            return self._tokenize_single(text)
    
    def _tokenize_single(self, text):
        """Tokenize single medical report"""
        preprocessed = self._preprocess_medical_text(text)
        
        # Use base tokenizer
        tokens = self.base_tokenizer.encode(
            preprocessed,
            max_length=512,
            truncation=True,
            return_tensors="pt"
        )
        
        return tokens.squeeze(0)

# Use with Nolano.AI
medical_tokenizer = MedicalReportTokenizer()

def build() -> DataPreparationConfig:
    return DataPreparationConfig(
        input_path="./medical_reports.jsonl",
        output_path="./prepared_medical",
        tokenization=medical_tokenizer,
        max_sequence_length=512
    )
```

## Best Practices

<CardGroup cols={2}>
  <Card 
    title="Vocabulary Size" 
    icon="list"
    color="#935095"
  >
    Choose vocabulary size based on your data complexity and memory constraints
  </Card>
  <Card 
    title="Special Tokens" 
    icon="tag"
    color="#B47BB6"
  >
    Always include padding, unknown, start, and end tokens
  </Card>
  <Card 
    title="Preprocessing" 
    icon="filter"
    color="#6F3A71"
  >
    Handle domain-specific patterns before general tokenization
  </Card>
  <Card 
    title="Testing" 
    icon="check"
    color="#935095"
  >
    Validate tokenization and detokenization are invertible
  </Card>
</CardGroup>

<Tip>
  **Performance**: Custom tokenizers should return PyTorch tensors for optimal performance with Nolano.AI.
</Tip>

<Warning>
  **Compatibility**: Ensure your tokenizer handles edge cases like empty inputs, very long sequences, and special characters.
</Warning>
